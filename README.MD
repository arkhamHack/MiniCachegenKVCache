# ğŸš€ MiniCacheGenKVCache â€” Streaming & Compressed KV Cache with RoPE from Scratch

> A modern, research-inspired Key/Value caching engine for Transformers â€” built from scratch with RoPE, streaming, and token-wise compression inspired by [CacheGen (2023)](https://arxiv.org/abs/2310.07240).  

âš¡ï¸ Ideal for understanding LLM inference optimizations.  
ğŸ¯ Easily pluggable into custom models or demo apps.  
ğŸ§  Educational + performant = blog-worthy!

---

## ğŸ” What's inside?

- ğŸ§  **RoPE-Integrated KV Cache**  
  Rotary positional embeddings applied correctly to `Q` and `K`.

- ğŸ’¾ **MiniCacheGen Compression**  
  Compresses KV cache using:
  - Anchor + delta token strategy  
  - Quantized per-layer encoding (4/6/8 bit)  
  - Decompression on demand

- ğŸ“¦ **Streaming Support**  
  Inference-friendly architecture: cache grows with tokens, compressed in chunks.

- ğŸ”¬ **Benchmark-Ready**  
  Compare:
  - Uncompressed KV cache vs MiniCacheGenKVCache  
  - Speed & memory use  
  - Quality (optionally)
---

## ğŸ§ª Quickstart

### 1. Clone + install
```bash
git clone https://github.com/your-username/minicachegen-kvcache.git
cd minicachegen-kvcache
pip install -r requirements.txt
